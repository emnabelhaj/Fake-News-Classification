{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"latex_envs":{"LaTeX_envs_menu_present":true,"autoclose":true,"autocomplete":true,"bibliofile":"biblio.bib","cite_by":"apalike","current_citInitial":1,"eqLabelWithNumbers":true,"eqNumInitial":1,"hotkeys":{"equation":"Ctrl-E","itemize":"Ctrl-I"},"labels_anchors":false,"latex_user_defs":false,"report_style_numbering":false,"user_envs_cfg":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false},"colab":{"name":"Real_VS_Fake_News.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Library Imports","metadata":{"id":"FfJltntgdDAN"}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nimport re\nfrom wordcloud import WordCloud","metadata":{"id":"KsRqEzY_dDAZ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Embedding, LSTM, Conv1D, MaxPool1D\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"id":"m5Ac0QZWo-KD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring Fake News","metadata":{"id":"YapFgrosdDBf"}},{"cell_type":"code","source":"fake = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/Fake.csv\")","metadata":{"id":"_bNLlXdWdDBn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake.head()","metadata":{"id":"3arpkQWfdDCS","outputId":"84c7ffa4-b00b-414e-d1b2-5b8d83d9e2c8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Counting by Subjects \nfor key,count in fake.subject.value_counts().iteritems():\n    print(f\"{key}:\\t{count}\")\n    \n#Getting Total Rows\nprint(f\"Total Records:\\t{fake.shape[0]}\")","metadata":{"id":"CXeCGdsAdDDC","outputId":"1892de64-c58a-4035-b5e6-cefaac48ddbc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(8,5))\nsns.countplot(\"subject\", data=fake)\nplt.show()","metadata":{"id":"tt3u2XJ5dDDs","outputId":"c4ab6c8d-91e1-4463-96b2-44bb7b21b502","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Word Cloud\ntext = ''\nfor news in fake.text.values:\n    text += f\" {news}\"\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text","metadata":{"id":"6SL6XlPCdDEp","outputId":"aa585d7d-1d14-427e-e996-1cf81e428c77","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Exploring Real news","metadata":{"id":"7wjJakMydDF_"}},{"cell_type":"code","source":"real = pd.read_csv(\"/kaggle/input/fake-and-real-news-dataset/True.csv\")\nreal.head()","metadata":{"id":"ns94H6hsdDGL","outputId":"b7b61aa4-088a-4ccc-960c-4beb9485ca14","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#First Creating list of index that do not have publication part\nunknown_publishers = []\nfor index,row in enumerate(real.text.values):\n    try:\n        record = row.split(\" -\", maxsplit=1)\n        #if no text part is present, following will give error\n        record[1]\n        #if len of piblication part is greater than 260\n        #following will give error, ensuring no text having \"-\" in between is counted\n        assert(len(record[0]) < 260)\n    except:\n        unknown_publishers.append(index)","metadata":{"id":"wTxfBZkedDH2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Thus we have list of indices where publisher is not mentioned\n#lets check\nreal.iloc[unknown_publishers].text\n#true, they do not have text like \"WASHINGTON (Reuters)\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Seperating Publication info, from actual text\npublisher = []\ntmp_text = []\nfor index,row in enumerate(real.text.values):\n    if index in unknown_publishers:\n        #Add unknown of publisher not mentioned\n        tmp_text.append(row)\n        \n        publisher.append(\"Unknown\")\n        continue\n    record = row.split(\" -\", maxsplit=1)\n    publisher.append(record[0])\n    tmp_text.append(record[1])","metadata":{"id":"Xw7Ec258dDIx","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Replace existing text column with new text\n#add seperate column for publication info\nreal[\"publisher\"] = publisher\nreal[\"text\"] = tmp_text\n\ndel publisher, tmp_text, record, unknown_publishers","metadata":{"id":"ieCzo5pYdDJU","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"real.head()","metadata":{"id":"lGqTE2qtdDJo","outputId":"e7600469-ff61-4c3e-b35e-89fd0804099f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"New column called \"Publisher\" has been added.\n","metadata":{}},{"cell_type":"code","source":"#checking for rows with empty text like row:8970\n[index for index,text in enumerate(real.text.values) if str(text).strip() == '']\n#seems only one :)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#dropping this record\nreal = real.drop(8970, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# checking for the same in fake news\nempty_fake_index = [index for index,text in enumerate(fake.text.values) if str(text).strip() == '']\nprint(f\"No of empty rows: {len(empty_fake_index)}\")\nfake.iloc[empty_fake_index].tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**630 Rows in Fake news with empty text**\n\nFake news have a lot of CPATIAL-CASES. Because we will use word2vec vectors later on, which haswell-formed lower cases word, we will contert to lower case.\n\nThe text for these rows seems to be present in title itself, so we'll merge title and text to solve these cases.","metadata":{}},{"cell_type":"code","source":"#Getting Total Rows\nprint(f\"Total Records:\\t{real.shape[0]}\")\n\n#Counting by Subjects \nfor key,count in real.subject.value_counts().iteritems():\n  print(f\"{key}:\\t{count}\")","metadata":{"id":"Ncknf-UPdDKQ","outputId":"f83f054e-1be0-4dab-d5f8-7d95a4e02257","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.countplot(x=\"subject\", data=real)\nplt.show()","metadata":{"id":"F7hyf09XdDKg","outputId":"668ffd97-03e1-4ed8-fc5d-08c2e7e3f2b5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#WordCloud For Real News\ntext = ''\nfor news in real.text.values:\n    text += f\" {news}\"\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(str(text))\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text","metadata":{"id":"8vZOUiSJdDKz","outputId":"10039276-931d-4aa1-9d87-c8c497a345be","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Text","metadata":{"id":"31IETzH6dDLR"}},{"cell_type":"code","source":"# Adding class Information\nreal[\"class\"] = 1\nfake[\"class\"] = 0","metadata":{"id":"dJpU7O2OdDLT","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combining Title and Text\nreal[\"text\"] = real[\"title\"] + \" \" + real[\"text\"]\nfake[\"text\"] = fake[\"title\"] + \" \" + fake[\"text\"]","metadata":{"id":"SYj4Dnm7dDLh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Subject is diffrent for real and fake thus dropping it\n# Aldo dropping Date, title and Publication Info of real\nreal = real.drop([\"subject\", \"date\",\"title\",  \"publisher\"], axis=1)\nfake = fake.drop([\"subject\", \"date\", \"title\"], axis=1)","metadata":{"id":"jiBm_dDWdDLw","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Combining both into new dataframe\ndata = real.append(fake, ignore_index=True)\ndel real, fake","metadata":{"id":"5_L4CePKdDME","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download following if not downloaded in local machine\n\n# nltk.download('stopwords')\n# nltk.download('punkt')","metadata":{"id":"R74pjsDYkz34","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Removing StopWords, Punctuations and single-character words","metadata":{}},{"cell_type":"code","source":"y = data[\"class\"].values\n#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process\nX = []\nstop_words = set(nltk.corpus.stopwords.words(\"english\"))\ntokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nfor par in data[\"text\"].values:\n    tmp = []\n    sentences = nltk.sent_tokenize(par)\n    for sent in sentences:\n        sent = sent.lower()\n        tokens = tokenizer.tokenize(sent)\n        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]\n        tmp.extend(filtered_words)\n    X.append(tmp)\n\ndel data","metadata":{"id":"H07kA_z6dDMX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Vectorization using Word2Vec\n\n\n\n\n","metadata":{"id":"ZFLUPXGodDMv"}},{"cell_type":"markdown","source":"#### We will create and check our own Word2Vec model with **gensim**","metadata":{"id":"JHKIgurRdDM2"}},{"cell_type":"code","source":"import gensim","metadata":{"id":"GoQRvjexdDM5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dimension of vectors we are generating\nEMBEDDING_DIM = 100\n\n#Creating Word Vectors by Word2Vec Method (takes time...)\nw2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)","metadata":{"id":"8JpCLfaldDNW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vocab size\nlen(w2v_model.wv.vocab)\n\n#We have now represented each of 122248 words by a 100dim vector.","metadata":{"id":"DP1nyaGqdDNr","outputId":"f45e5906-dc2b-448b-fc38-f3fca9901e1f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring Vectors\n\nLets checkout these vectors","metadata":{"id":"iloSEX4NdDN_"}},{"cell_type":"code","source":"#see a sample vector for random word, lets say Corona \nw2v_model[\"corona\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"iran\")","metadata":{"id":"s3eymov_dDOC","outputId":"781b409b-421f-4284-bc0c-c8952db524fd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"fbi\")","metadata":{"id":"iLwtmvpydDOP","outputId":"ba2f4e47-1f96-4fd4-fe7d-a1740f684b88","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"facebook\")","metadata":{"id":"UCYch1mUt1Cn","outputId":"06447659-bcdf-4ee4-8d08-4888a3cb43d5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w2v_model.wv.most_similar(\"computer\")","metadata":{"id":"ErNpsa8Lt8Vi","outputId":"d887ca8d-70e5-4f2b-9578-f8cdfe94f769","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Feeding US Presidents\nw2v_model.wv.most_similar(positive=[\"trump\",\"obama\", \"clinton\"])\n#First was Bush","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Looking at the similar words, vectors are well formed for these words **\n\n\nThese Vectors will be passed to LSTM/GRU instead of words. 1D-CNN can further be used to extract features from the vectors. \n\n\nKeras has implementation called \"**Embedding Layer**\" which would create word embeddings(vectors). Since we did that with gensim's word2vec, we will load these vectors into embedding layer and make the layer non-trainable.\n\n\n","metadata":{"id":"1jDnzu5Htzsp"}},{"cell_type":"markdown","source":"We cannot pass string words to embedding layer, thus need some way to represent each words by numbers.\n\nTokenizer can represent each word by number","metadata":{}},{"cell_type":"code","source":"# Tokenizing Text -> Repsesenting each word by a number\n# Mapping of orginal word to number is preserved in word_index property of tokenizer\n\n#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(X)\n\nX = tokenizer.texts_to_sequences(X)","metadata":{"id":"8nQ8aN_brt-m","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets check the first 10 words of first news\n#every word has been represented with a number\nX[0][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets check few word to numerical replesentation\n#Mapping is preserved in dictionary -> word_index property of instance\nword_index = tokenizer.word_index\nfor word, num in word_index.items():\n    print(f\"{word} -> {num}\")\n    if num == 10:\n        break        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can pass numerical representation of words into neural network.\n\nWe can use Many-To-One (Sequence-To-Word) Model of RNN, as we have many words in news as input and one output ie Probability of being Real.\n\nFor Many-To-One model, we  use a fixed size input. \n","metadata":{}},{"cell_type":"code","source":"# For determining size of input...\n\n# Making histogram for no of words in news shows that most news article are under 700 words.\n# Lets keep each news small and truncate all news to 700 while tokenizing\nplt.hist([len(x) for x in X], bins=500)\nplt.show()\n\n# Its heavily skewed. There are news with 5000 words? Lets truncate these outliers :) ","metadata":{"id":"xQYGbmRZrtrO","outputId":"65ebe7a7-944b-4331-d300-41b86922377b","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nos = np.array([len(x) for x in X])\nlen(nos[nos  < 700])\n# Out of 48k news, 44k have less than 700 words","metadata":{"id":"ObfiqLhyrtxY","outputId":"544f333e-5642-426c-d460-ce0326789d5f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Lets keep all news to 700, add padding to news with less than 700 words and truncating long ones\nmaxlen = 700 \n\n#Making all news of size maxlen defined above\nX = pad_sequences(X, maxlen=maxlen)","metadata":{"id":"qf-4aQnqrt6M","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#all news has 700 words (in numerical form now). If they had less words, they have been padded with 0\n# 0 is not associated to any word, as mapping of words started from 1\n# 0 will also be used later, if unknows word is encountered in test set\nlen(X[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Adding 1 because of reserved 0 index\n# Embedding Layer creates one more vector for \"UNKNOWN\" words, or padded words (0s). This Vector is filled with zeros.\n# Thus our vocab size inceeases by 1\nvocab_size = len(tokenizer.word_index) + 1","metadata":{"id":"0q3cFF-N2Mix","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to create weight matrix from word2vec gensim model\ndef get_weight_matrix(model, vocab):\n    # total vocabulary size plus 0 for unknown words\n    vocab_size = len(vocab) + 1\n    # define weight matrix dimensions with all 0\n    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n    # step vocab, store vectors using the Tokenizer's integer mapping\n    for word, i in vocab.items():\n        weight_matrix[i] = model[word]\n    return weight_matrix","metadata":{"id":"wFjoigbq43SM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We Create a matrix of mapping between word-index and vectors. We use this as weights in embedding layer\n\nEmbedding layer accepts numecical-token of word and outputs corresponding vercor to inner layer.\n\nIt sends vector of zeros to next layer for unknown words which would be tokenized to 0.\n\n\nInput length of Embedding Layer is the length of each news (700 now due to padding and truncating)","metadata":{}},{"cell_type":"code","source":"#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\nembedding_vectors = get_weight_matrix(w2v_model, word_index)","metadata":{"id":"o2etO3jIrtvM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))\n#LSTM \nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_vectors","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"B9QlV7NrID0_","outputId":"7388db21-77ab-4b6f-ac1d-ac3e30ec533c","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y) ","metadata":{"id":"II-pr_sgHpcX","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, validation_split=0.3, epochs=6)","metadata":{"id":"GkCiftes3JD6","outputId":"3741f776-453b-418f-9d91-d572f96cb720","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Prediction is in probability of news being real, so converting into classes\n# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)\ny_pred = (model.predict(X_test) >= 0.5).astype(\"int\")","metadata":{"id":"swVt7TjQzNCM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, y_pred)","metadata":{"id":"5l6MGjUTJntM","outputId":"60e58b71-85e3-4a57-8663-aacde39d6dee","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"id":"lAx6hEfbJqvm","outputId":"5c5b56cc-acd8-41b2-e9d8-4e5114fa7ba5","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"del model","metadata":{"id":"8ncDQMKRyE55","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Pre-Trained Word2Vec Vectors\n\n","metadata":{"id":"cC8v4HlZdLcu"}},{"cell_type":"markdown","source":"Now, instead of creating word vectors, let us use pre-trained vectors trained on part of **Google News dataset** (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases.  Source: https://code.google.com/archive/p/word2vec/\n\n**Please download model file from**: https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing\n\n\nOr add Dataset from https://www.kaggle.com/sandreds/googlenewsvectorsnegative300\n","metadata":{"id":"u5k8KpZGKfTf"}},{"cell_type":"code","source":"#invoke garbage collector to free ram\nimport gc\ngc.collect()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from gensim.models.keyedvectors import KeyedVectors","metadata":{"id":"qk_AEI94LmHI","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Takes RAM \nword_vectors = KeyedVectors.load_word2vec_format('../input/googlenewsvectorsnegative300/GoogleNews-vectors-negative300.bin', binary=True)\nEMBEDDING_DIM=300","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploring these trained Vectors","metadata":{"id":"iLcTApQpgXpe"}},{"cell_type":"code","source":"# word_vectors.most_similar('usa')","metadata":{"id":"esvjlzgzgUH5","outputId":"b5d723c4-4b1e-4268-d2e0-fbeef1df06bb","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_vectors.most_similar('fbi')","metadata":{"id":"C2ZIES6SgUMr","outputId":"e9c3f7f4-0888-4294-e650-e5c6c24d34c2","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# word_vectors.most_similar('Republic')","metadata":{"id":"HbRzkWt9gUaJ","outputId":"c9d4ce2f-a371-4e33-c47f-adf91b402e08","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    try:\n        embedding_vector = word_vectors[word]\n        embedding_matrix[i] = embedding_vector\n    except KeyError:\n        embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n\ndel word_vectors ","metadata":{"id":"sdkVbKXqfJGh","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Sequential()\nmodel.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_matrix], input_length=maxlen, trainable=False))\nmodel.add(Conv1D(activation='relu', filters=4, kernel_size=4))\nmodel.add(MaxPool1D())\nmodel.add(LSTM(units=128))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n\ndel embedding_matrix","metadata":{"id":"UoSJl84gLZi8","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"id":"fzvS8m5LLZtu","outputId":"118799c3-468b-4547-cdec-dbd1a196e518","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train, y_train, validation_split=0.3, epochs=12)","metadata":{"id":"OMJklhVAjNGZ","outputId":"1cb4a101-3a36-4563-a40a-9ccfaf12c945","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = (model.predict(X_test) > 0.5).astype(\"int\")","metadata":{"id":"ZaUCmKYojQJS","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy_score(y_test, y_pred)","metadata":{"id":"CkZtLGNYjXnu","outputId":"5562a291-d9f5-4e7d-9ea1-bd76cabba413","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, y_pred))","metadata":{"id":"FTqJSkXSjXrf","outputId":"5809f01a-3a4d-4c3e-8f29-534206ce23dc","trusted":true},"execution_count":null,"outputs":[]}]}